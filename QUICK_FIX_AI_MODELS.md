# ðŸš€ Quick Fix Guide - AI Model Connections

## What Was Fixed

âœ… **5 service configuration files** updated with AI provider settings  
âœ… **LocalAI models.yaml** reconfigured to bridge Ollama  
âœ… **1 missing config file** created (curriculum-content-svc)  
âœ… **Test suite** created to verify everything works

---

## Run These Commands (In Order)

### 1. Start Docker Desktop
**IMPORTANT**: Docker must be running first!

Check if running:
```powershell
docker version
```

If error â†’ Open Docker Desktop app and wait for it to start.

---

### 2. Start All Services
```powershell
cd C:\Users\ofema\aivo-pro
docker-compose up -d
```

Wait 30 seconds for services to initialize.

---

### 3. Install AI Models (First Time Only)
```powershell
# Install all required models (~15GB, takes 5-10 minutes)
.\scripts\setup-local-ai.ps1
```

**Models being installed**:
- llama3.2:3b (2GB) - Main chat model
- codellama:7b (4GB) - Code generation
- llava:7b (4.5GB) - Vision/image understanding
- nomic-embed-text (274MB) - Text embeddings
- tinyllama:1.1b (637MB) - Fallback model

---

### 4. Run Test Suite
```powershell
.\scripts\test-ai-models.ps1
```

**Expected**: All tests should pass âœ…

---

## Quick Verification

### Check if services are running:
```powershell
docker ps
```

You should see these containers:
- aivo-pro-ollama-1
- aivo-pro-localai-1
- aivo-pro-aivo-brain-1
- aivo-pro-homework-helper-1
- aivo-pro-curriculum-content-1
- aivo-pro-iep-assistant-1
- aivo-pro-model-cloning-1

### Test a model manually:
```powershell
# Test Ollama
docker exec aivo-pro-ollama-1 ollama run llama3.2:3b "Say hello"

# Test via API
curl http://localhost:11434/api/version
curl http://localhost:8080/readyz
curl http://localhost:8014/health
```

---

## If Something Goes Wrong

### Container won't start?
```powershell
docker-compose logs <service-name>
docker-compose restart <service-name>
```

### Model not found?
```powershell
docker exec aivo-pro-ollama-1 ollama list
docker exec aivo-pro-ollama-1 ollama pull llama3.2:3b
```

### Service returns errors?
```powershell
# View logs
docker-compose logs -f aivo-brain
docker-compose logs -f homework-helper

# Restart all services
docker-compose restart
```

---

## What Changed

| File | Change |
|------|--------|
| `services/homework-helper-svc/src/config.py` | Added AI provider config |
| `services/iep-assistant-svc/src/config.py` | Added AI provider config |
| `services/aivo-brain-svc/src/config.py` | Updated model URLs |
| `services/model-cloning-svc/src/config.py` | Added AI provider config |
| `services/curriculum-content-svc/src/config.py` | **CREATED NEW FILE** |
| `docker/localai/models.yaml` | Updated to bridge Ollama |
| `scripts/test-ai-models.ps1` | **CREATED TEST SUITE** |

---

## Access Points After Setup

| Service | URL | Purpose |
|---------|-----|---------|
| Ollama Web UI | http://localhost:3000 | Test models visually |
| Ollama API | http://localhost:11434 | Direct model access |
| LocalAI | http://localhost:8080 | OpenAI-compatible API |
| AIVO Brain | http://localhost:8014 | Main AI service |
| Homework Helper | http://localhost:8007 | Homework assistance |

---

## Summary

**Before**: Services had no way to connect to local AI models  
**After**: All services configured to use Ollama via LocalAI bridge  

**Time to fix**: ~5 minutes (excluding model downloads)  
**Cost**: $0 (vs $30-100/month for cloud AI)  
**Quality**: Production-ready with real AI models

---

**Need more details?** See `AI_MODEL_CONNECTION_FIXES.md`
